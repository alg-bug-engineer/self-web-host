---
title: 用人话讲 Transformer：从注意力机制到 GPT
description: 不用公式，用直觉理解 Transformer 的核心思想。这篇文章会用最简单的语言解释为什么 Transformer 能够改变 AI 世界。
date: 2025-02-01
author: 芝士AI吃鱼
tags:
  - Transformer
  - 深度学习
  - NLP
icon: cat
published: true
---

## 为什么要了解 Transformer？

如果说深度学习是 AI 的「发动机」，那 Transformer 就是让这台发动机真正飞起来的「涡轮增压器」。

<InfoCard type="robot" title="Transformer 的地位">
  ChatGPT、Claude、Gemini... 几乎所有你听说过的大语言模型，底层都是 Transformer 架构。
</InfoCard>

## 从一个问题开始

假设你要翻译这句话：

> The cat sat on the mat because **it** was tired.

这里的「it」指的是什么？是 cat 还是 mat？

<InfoCard type="cat">
  🐱 猫说：当然是指我啦！mat 怎么会累呢？
</InfoCard>

人类能轻松理解这个指代关系，但对于计算机来说，这并不容易。要理解「it」，模型需要「回头看」前面的内容，找到相关的词。

**这就是「注意力」的核心思想：让模型能够关注输入中最相关的部分。**

## 注意力机制是什么？

想象你在嘈杂的派对上和朋友聊天：

<TwoColumnLayout>
  <Left>
    ### 没有注意力
    你听到所有人的声音，混在一起，根本听不清朋友在说什么。
  </Left>
  <Right>
    ### 有注意力
    你的大脑自动「过滤」掉噪音，专注于朋友的声音。
  </Right>
</TwoColumnLayout>

Transformer 的注意力机制做的事情类似：

<Step number={1} title="计算相关性">
  对于每个词，计算它和其他所有词的「相关程度」
</Step>

<Step number={2} title="分配注意力">
  相关程度高的词，获得更多「注意力权重」
</Step>

<Step number={3} title="加权融合">
  根据注意力权重，把相关信息融合在一起
</Step>

## 自注意力（Self-Attention）

Transformer 的核心是「自注意力」机制。为什么叫「自」注意力？

因为它让句子中的每个词都「看」其他所有的词，包括它自己。

<CodeBlock filename="伪代码示意">
```
对于句子中的每个词 word_i:
    对于句子中的每个词 word_j:
        计算 word_i 对 word_j 的注意力分数
    把所有注意力分数归一化
    用注意力分数加权求和，得到 word_i 的新表示
```
</CodeBlock>

<InfoCard type="tip" title="为什么这很强大？">
  传统的 RNN 是「一个一个词」处理的，长距离的信息很难传递。

  而自注意力让每个词都能「直接看到」其他所有词，不管距离多远！
</InfoCard>

## Transformer 的结构

完整的 Transformer 由两部分组成：

<TwoColumnLayout>
  <Left>
    ### 编码器 (Encoder)
    理解输入内容，生成中间表示

    **代表模型**：BERT
  </Left>
  <Right>
    ### 解码器 (Decoder)
    基于中间表示，生成输出

    **代表模型**：GPT
  </Right>
</TwoColumnLayout>

<HighlightBox>
  **有趣的事实**：GPT 系列只用了解码器部分，而 BERT 只用了编码器部分。原版 Transformer 是为机器翻译设计的，所以两部分都用上了。
</HighlightBox>

## 为什么 Transformer 这么成功？

<Step number={1} title="并行计算">
  RNN 必须一个词一个词处理，而 Transformer 可以同时处理所有词，训练速度快几十倍
</Step>

<Step number={2} title="长距离依赖">
  自注意力让模型能轻松捕捉长距离的依赖关系
</Step>

<Step number={3} title="可扩展性">
  模型越大、数据越多，效果越好（scaling law）
</Step>

## 总结

<InfoCard type="robot" title="一句话总结">
  Transformer 的核心是「注意力机制」，它让模型能够灵活地关注输入中最相关的部分，不受距离限制，还能并行计算。
</InfoCard>

下一篇文章，我们会用代码实现一个简化版的自注意力机制。敬请期待！

---

<Quote author="Attention Is All You Need 论文">
  The Transformer is the first transduction model relying entirely on self-attention.
</Quote>
